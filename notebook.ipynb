{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "\n",
    "# `reviews` is a list of review strings, and `labels` is a list of polarity values\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "# Open the CSV file\n",
    "with open('EcoPreprocessed.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)  # Using DictReader to read CSV as dictionaries\n",
    "    for row in reader:\n",
    "        reviews.append(row['review'])  # Extract the 'review' column\n",
    "        labels.append(float(row['polarity']))  # Convert the 'polarity' column to float and append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a set of all unique words in the dataset\n",
    "words = {word for sentence in reviews for word in sentence.split()}\n",
    "\n",
    "# Build a mapping from words to integers based on lexicographical ordering\n",
    "word_to_int = {word: idx + 1 for idx, word in enumerate(sorted(words))}\n",
    "\n",
    "# Encode each sentence to a list of integers based on the word_to_int mapping\n",
    "def encode(sentence):\n",
    "    return [word_to_int.get(word, 0) for word in sentence.split()]  # Default to 0 if word not found\n",
    "\n",
    "# Convert each sentence into a tensor of integers\n",
    "var_len_tensors = [torch.tensor(encode(sentence)) for sentence in reviews]\n",
    "\n",
    "# Pad the sequences to the same length\n",
    "training_dataset = nn.utils.rnn.pad_sequence(var_len_tensors, batch_first=True)\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(words) + 1  # Adding 1 to account for the padding token (0)\n",
    "\n",
    "# Convert labels to tensor\n",
    "training_labels = torch.unsqueeze(torch.tensor(labels), dim=-1)\n",
    "\n",
    "# Now we have vocabulary_size, training_dataset, word_to_int, and training_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear_layer = nn.Linear(embedding_dim, 1) # Fully connected layer into a single neuron\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Lookup embeddings for the input tokens\n",
    "        embedded = self.embedding_layer(x)\n",
    "        # Compute the mean of the embeddings across the sequence length (dim=1)\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        # Project the pooled representation to a single output score\n",
    "        output = self.linear_layer(pooled)\n",
    "        # Apply activation function Tanh to output between -1 and 1\n",
    "        return self.tanh(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Loss: 1.2384\n",
      "Epoch [100/1000], Loss: 0.1590\n",
      "Epoch [200/1000], Loss: 0.0953\n",
      "Epoch [300/1000], Loss: 0.1296\n",
      "Epoch [400/1000], Loss: 0.1100\n",
      "Epoch [500/1000], Loss: 0.0843\n",
      "Epoch [600/1000], Loss: 0.0829\n",
      "Epoch [700/1000], Loss: 0.1094\n",
      "Epoch [800/1000], Loss: 0.0918\n",
      "Epoch [900/1000], Loss: 0.0838\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "embedding_dimension = 256\n",
    "batch_size = 64\n",
    "num_epochs = 1000\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = EmotionPredictor(vocab_size, embedding_dimension)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle dataset at the start of each epoch\n",
    "    randperm = torch.randperm(len(training_dataset))\n",
    "    training_dataset, training_labels = training_dataset[randperm], training_labels[randperm]\n",
    "    # Take a batch of the first `batch_size` samples\n",
    "    mini_batch = training_dataset[:batch_size]\n",
    "    mini_batch_labels = training_labels[:batch_size]\n",
    "\n",
    "    # Forward pass\n",
    "    prediction = model(mini_batch)\n",
    "    # Compute loss\n",
    "    loss = loss_function(prediction, mini_batch_labels)\n",
    "    # Backpropagation and optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5090, 2955, 1530]])\n",
      "[[-0.9994620084762573]]\n"
     ]
    }
   ],
   "source": [
    "# Define the examples\n",
    "examples = [\n",
    "    \"worst movie ever\",\n",
    "    \"best movie ever\",\n",
    "    \"weird but funny movie\"\n",
    "]\n",
    "\n",
    "# Encode the examples as integer tensors using the word_to_int mapping\n",
    "encoded_examples = [\n",
    "    torch.tensor([word_to_int.get(word, 0) for word in example.split()])\n",
    "    for example in examples\n",
    "]\n",
    "\n",
    "# Pad the sequences to make them the same length (for batching)\n",
    "testing_tensor = torch.nn.utils.rnn.pad_sequence(encoded_examples, batch_first=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform the prediction\n",
    "predictions = model(testing_tensor)\n",
    "\n",
    "# Print the predictions as a list\n",
    "print(predictions.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Long(1, *, strides=[50, 1], requires_grad=0, device=cpu),\n",
      "      %embedding_layer.weight : Float(5159, 256, strides=[256, 1], requires_grad=1, device=cpu),\n",
      "      %linear_layer.weight : Float(1, 256, strides=[256, 1], requires_grad=1, device=cpu),\n",
      "      %linear_layer.bias : Float(1, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/embedding_layer/Gather_output_0 : Float(1, *, 256, strides=[12800, 256, 1], requires_grad=1, device=cpu) = onnx::Gather[onnx_name=\"/embedding_layer/Gather\"](%embedding_layer.weight, %input), scope: __main__.EmotionPredictor::/torch.nn.modules.sparse.Embedding::embedding_layer # /Users/tushar/VSCodeProjects/Jupyter Notebooks/sentiment-analysis/.venv/lib/python3.13/site-packages/torch/nn/functional.py:2551:0\n",
      "  %/ReduceMean_output_0 : Float(1, 256, strides=[256, 1], requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[1], keepdims=0, onnx_name=\"/ReduceMean\"](%/embedding_layer/Gather_output_0), scope: __main__.EmotionPredictor:: # /var/folders/v8/wg_0wzqx6lsb8sj_wxm8xvj00000gn/T/ipykernel_18667/3807090486.py:12:0\n",
      "  %/linear_layer/Gemm_output_0 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/linear_layer/Gemm\"](%/ReduceMean_output_0, %linear_layer.weight, %linear_layer.bias), scope: __main__.EmotionPredictor::/torch.nn.modules.linear.Linear::linear_layer # /Users/tushar/VSCodeProjects/Jupyter Notebooks/sentiment-analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %7 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = onnx::Tanh[onnx_name=\"/tanh/Tanh\"](%/linear_layer/Gemm_output_0), scope: __main__.EmotionPredictor::/torch.nn.modules.activation.Tanh::tanh # /Users/tushar/VSCodeProjects/Jupyter Notebooks/sentiment-analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/activation.py:392:0\n",
      "  return (%7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randint(0, vocab_size, (1, 50))\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    model,                    # model to export\n",
    "    dummy_input,              # dummy input (shape must match your model's input)\n",
    "    \"emotion_predictor.onnx\",          # path to save the ONNX file\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `word_to_int` Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "with open(\"word_to_int.json\", \"w\") as json_file:\n",
    "    json.dump(word_to_int, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
